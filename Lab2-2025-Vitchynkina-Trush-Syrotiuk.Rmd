---
title: 'P&S-2025: Lab assignment 2'
author: "Sofiia Vitchynkina, Sofiia Trush, Viktor Syrotiuk"
output:
  html_document:
    df_print: paged
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

------------------------------------------------------------------------

**Sofiia Vitchynkina : Task 1**

**Sofiia Trush : Task 3 and Task 4**

**Viktor Syrotiuk : Task 2**

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

#### You can (but do not have to) use the chunks we prepared for you

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
id <- 15

set.seed(id)
p <- id/100
# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- matrix(c(
  1,0,0,
  0,1,0,
  1,1,0,
  0,0,1,
  1,0,1,
  0,1,1,
  1,1,1
), nrow = 7, byrow = TRUE)
cat("The matrix G is: \n") 
G  
cat("The matrix H is: \n") 
H
cat("The product GH must be zero: \n")
(G%*%H) %%2
```

#### Next, generate the messages

```{r}
N = 10000

message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  
messages <- message_generator(N)
codewords <- (messages %*% G) %% 2
```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}
errors   <- matrix(rbinom(N * 7, size = 1, prob = p), nrow = N)
received <- (codewords + errors) %% 2 
```

The next steps include detecting the errors in the received messages, correcting them, and then decoding the obtained messages. After this, you can continue with calculating all the quantities of interest

```{r}
synd <- (received %*% H) %% 2
idx  <- synd[, 1] + 2 * synd[, 2] + 4 * synd[, 3]


corrected <- received
mask <- idx >= 1 & idx <= 7
r_idx <- which(mask)
c_idx <- idx[mask]
if (length(r_idx)) {
  corrected[cbind(r_idx, c_idx)] <- (corrected[cbind(r_idx, c_idx)] + 1) %% 2
}

decoded <- cbind(corrected[, 3], corrected[, 5], corrected[, 6], corrected[, 7])  # N x 4

success_vec <- as.integer(rowSums(decoded != messages) == 0)
errs_vec    <- rowSums(decoded != messages)
```

```{r}
phat <- mean(success_vec)
phat


pstar_theor <- (1 - p)^7 + 7 * p * (1 - p)^6
c(phat = phat, pstar_theor = pstar_theor, abs_diff = abs(phat - pstar_theor))


z  <- qnorm(0.975)
se <- sqrt(phat * (1 - phat) / N)
eps <- z * se 


c(se = se, eps = eps,
  CI_low = max(0, phat - eps),
  CI_high = min(1, phat + eps))

N_needed <- ceiling((z^2 * 0.25) / (0.03^2))
N_needed
```

## Justifiacation

p\^ - sample mean of Bernoulli outcomes(success = 1, failure = 0)

By the Law of Large Numbers\< the sample mean converges to the true probability p\* as N increases.

By the Central Limit Theorem, for large N the distribution of p\^ is normal

p\^ \~ (p*, p*(1-p\*) / N)

This allows to build the 95% confidence interval

p\^ +- sqrt(p^(1-p^)/N)

Theoretical value: p\* = (1 - p)\^7 + 7p \* (1-p)\^6

,because the code can correct only one bit error.

```{r}
hist(errs_vec,
     breaks = seq(-0.5, 4.5, by = 1),
     probability = TRUE,
     col = "lightblue",
     main = "Number of errors in the decoded 4-bit message",
     xlab = "Number of bit errors")
```

The number of wrong bits after decoding does not follow a standard distribution. Even though transmission errors are independent, the Hamming decoding process makes the output bits dependent on each other. Because of that, this random variable is not binomial — the condition of independence between trials is not satisfied.

## Conclusion

The simulation shows that the probability p\^ of correct transmission is almost the same as the theoretical value p∗

This happens because the bit error probability p = 0.15 is relatively low, so most messages are transmitted without errors or with only one error, which the Hamming code can correct.

The 95% confidence interval is narrow, meaning our estimate is reliable when the number of simulations N is large. The histogram confirms this: most decoded messages have zero errors, and only a few have one or two, as expected for a code that corrects single-bit errors.

Overall, the results match the theory and make sense for an error-correcting code with a low noise level.

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}
T_years <- 30.1
T <- T_years * 365 * 24 * 3600
lambda <- log(2) / T

id <- 15
m <- id * 10^-6

M <- 137
N_A <- 6 * 10^23

N <- (m / M) * N_A

mu <- N * lambda

K <- 10^3
n <- 5

set.seed(123)
sample_means <- colMeans(matrix(rpois(n * K, lambda = mu), nrow = n))

cat("Half-life period (s):", T, "\n")
cat("Lambda (decay probability per nucleus per second):", lambda, "\n")
cat("Number of atoms N:", N, "\n")
cat("Poisson parameter mu:", mu, "\n")


```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu_theor <- mu
sigma_theor <- sqrt(mu / n)

cat("Theoretical mean (mu_theor):", mu_theor, "\n")
cat("Theoretical standard deviation (sigma_theor):", sigma_theor, "\n")


```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu_theor - 3 * sigma_theor, mu_theor + 3 * sigma_theor)

Fs <- ecdf(sample_means)

plot(Fs,
     xlim = xlims,
     ylim = c(0, 1),
     col = "blue",
     lwd = 2,
     main = paste("Comparison of ECDF and CDF (n =", n, ")"),
     xlab = "Sample mean",
     ylab = "Cumulative probability")

curve(pnorm(x, mean = mu_theor, sd = sigma_theor),
      col = "red",
      lwd = 2,
      add = TRUE)

legend("bottomright",
       legend = c("Empirical ECDF", "Normal CDF"),
       col = c("blue", "red"),
       lwd = 2,
       lty = 1)

xx <- seq(xlims[1], xlims[2], length.out = 400)
diff <- max(abs(Fs(xx) - pnorm(xx, mean = mu_theor, sd = sigma_theor)))


cat("Max difference between ECDF and Normal CDF =", diff, "\n")

```

**Next, proceed with all the remaining steps**

```{r}
analyze_n <- function(n, mu, K = 10^3, seed = 123) {
  set.seed(seed)
  samp_means <- colMeans(matrix(rpois(n * K, lambda = mu), nrow = n))
  mu_theor <- mu
  sigma_theor <- sqrt(mu / n)

  xlims <- c(mu_theor - 3 * sigma_theor, mu_theor + 3 * sigma_theor)
  Fs <- ecdf(samp_means)

  plot(Fs,
       xlim = xlims, ylim = c(0, 1),
       col = "blue", lwd = 2,
       main = paste("ECDF vs Normal CDF (n =", n, ")"),
       xlab = "Sample mean", ylab = "Cumulative probability")
  curve(pnorm(x, mean = mu_theor, sd = sigma_theor),
        col = "red", lwd = 2, add = TRUE)
  legend("bottomright", legend = c("ECDF", "Normal CDF"),
         col = c("blue", "red"), lwd = 2, lty = 1)

  xx <- seq(xlims[1], xlims[2], length.out = 400)
  diff <- max(abs(Fs(xx) - pnorm(xx, mean = mu_theor, sd = sigma_theor)))
  cat("n =", n, " | Max ECDF–CDF diff =", diff, "\n")
  invisible(diff)
}

diff5  <- analyze_n(5,  mu)
diff10 <- analyze_n(10, mu)
diff50 <- analyze_n(50, mu)

```

```{r}
a <- 8 * 10^8

n_markov <- floor(0.05 * a / mu)

z <- 1.64485362695147
t <- (-z + sqrt(z^2 + 4 * a)) / 2
n_clt <- floor((t^2) / mu)

chernoff_ok <- function(n) {
  m <- n * mu
  if (m <= 0 || a < m) return(FALSE)
  y <- a / m
  bound <- exp(-m * (y * log(y) - y + 1))
  return(bound <= 0.05)
}

right <- floor(a / mu)
left  <- 1
best  <- 0
while (left <= right) {
  mid <- floor((left + right) / 2)
  if (chernoff_ok(mid)) {
    best <- mid
    left <- mid + 1
  } else {
    right <- mid - 1
  }
}
n_chernoff <- best

cat("Theoretical bounds for P(S_n < 8×10^8) >= 0.95\n")
cat("Markov bound  : n =", n_markov,  "\n")
cat("Chernoff bound: n =", n_chernoff, "\n")
cat("CLT bound     : n =", n_clt,     "\n")
cat("Comparison (Markov < Chernoff < CLT expected)\n")


```

```{r}
check_n <- function(n, mu, a, K = 10^4, seed = 123) {
  set.seed(seed)
  s <- rpois(K, lambda = n * mu)
  p_emp <- mean(s < a)
  cat("n =", n, "| Empirical P(S_n < a) =", p_emp, "\n")
  return(p_emp)
}

p_markov  <- check_n(n_markov,  mu, a)
p_chernoff<- check_n(n_chernoff,mu, a)
p_clt     <- check_n(n_clt,     mu, a)

```

#### Conclusion

The Poisson distribution with parameter μ = Nλ was successfully simulated for Cs-137 samples. For n = 5, 10 and 50 the sample-mean distributions became increasingly close to the normal N(μ, μ/n), and the maximal ECDF–CDF difference decreased as n grew, confirming the Central Limit Theorem. Theoretical bounds for n based on Markov, Chernoff and CLT methods were obtained. The Markov bound was very conservative, while Chernoff and CLT gave much tighter and similar limits. Simulation of P(Sₙ \< 8×10⁸) showed empirical probabilities close to 0.95, supporting the theoretical results. Overall, the experiment demonstrates that Poisson processes with large means are well approximated by the normal distribution, and that probabilistic bounds from Chernoff and CLT are practically reliable.

------------------------------------------------------------------------

### Task 3.

#### In this task, we use the Central Limit Theorem approximation for continuous random variables.

#### One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

1.  Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,
    -   simulate the realizations $x_1,x_2,\dots,x_n$ of the \textbf{r.v.} $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the \emph{empirical cumulative distribution} function $F_{\mathbf{s}}$ of $\mathbf{s}$;\
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $F_{\mathbf{s}}$ of and plot both \textbf{c.d.f.}'s on one graph to visualize their proximity;\
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;\
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.
2.  The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,
    -   express the event of interest in terms of the \textbf{r.v.} $S:= X_1 + \cdots + X_{100}$;\
    -   obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;\
    -   with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;\
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;\
    -   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

#### First, generate samples an sample means:

```{r}
id <- 15
v1 <- id + 10
K <- 1e3

n_values = c(5, 10, 50)

sample_means_n5 <- colMeans(matrix(rexp(n_values[1]*K, rate = v1), nrow=n_values[1]))
sample_means_n10 <- colMeans(matrix(rexp(n_values[2]*K, rate = v1), nrow=n_values[2]))
sample_means_n50 <- colMeans(matrix(rexp(n_values[3]*K, rate = v1), nrow=n_values[3]))

head(sample_means_n5)
head(sample_means_n10)
head(sample_means_n50)
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
for (n in n_values) {
  cat("For n =", n,":\n")
  mu <- 1 / v1
  sigma <- 1 / (v1 * sqrt(n))
  cat("μ =", mu, "\n")
  cat("σ =", sigma, "\n")
  cat("\n")
}
```

#### We can now plot ecdf and cdf

```{r}
for (n in n_values) {
  mu <- 1 / v1
  sigma <- 1 / (v1 * sqrt(n))
  xlims <- c(mu-3*sigma,mu+3*sigma)
  sample_means <- colMeans(matrix(rexp(n*K, rate = v1), nrow=n))
  Fs <- ecdf(sample_means)
  plot(Fs, 
       xlim = xlims, 
       col = "blue",
       lwd = 2,
       main = "Comparison of ecdf and cdf")
  curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
  legend("topleft", 
         legend = c("Empirical CDF", "CDF"),
         col = c("blue", "red"), 
         lwd = 2)
}
```

#### The maximal difference between the two\*\* \texbf{c.d.f}'s

```{r}
for (n in n_values) {
  mu <- 1 / v1
  sigma <- 1 / (v1 * sqrt(n))
  xlims <- c(mu - 3*sigma, mu + 3*sigma)
  sample_means <- colMeans(matrix(rexp(n * K, rate = v1), nrow = n))
  Fs <- ecdf(sample_means)
  x_vals <- seq(xlims[1], xlims[2], length.out = 200)
  max_diff <- max(abs(Fs(x_vals) - pnorm(x_vals, mean = mu, sd = sigma)))
  cat("Maximal difference for n =", n, ":", max_diff, "\n")
}

```

**Next, proceed with all the remaining steps**\

```{r}
id <- 15
v1 <- id + 10
# v1 = 25

v <- N * v1
# S = X1 + X2 + ..... + X100

mu_Xi <- 1 / v
sigma_Xi <- 1 / v

mu_S = 100 * mu_Xi
```

#### METHOD 1: Markov's Inequality - P(X ≥ a) ≤ E[X]​/a

**For our problem:**

P(S \> 60) ≥ 0.95 and it\`s the same if P(S \< 60) ≤ 0.05

But Markov's inequality works for P(X \> a) so we'll use:

P(S ≥ 60) ≥ 1 − P(S \< 60) ≥ 0.95

P(S \< 60) ≥ E(S)/60 = 100/v\*60 = 100 / (N \* 25) \* 60

P(S ≥ 60) ≥ 1 − 100 / (N \* 25) \* 60 ≥ 0.95

1 − 100 / (N \* 25) \* 60 ≥ 0.95

100 / (N \* 25) \* 60 ≤ 0.05

N ≤ 100 / 3 \* 25

=\> N ≤ 1.3333

#### METHOD 2: Chernoff bound

For the exponential distribution, the moment generating function (MGF) is: M_X(t) = ν / (ν - t) for t \< ν

P(S ≥ 60) = P(e\^(tS) ≥ e\^(60t)) ≤ E[e\^(tS)] / e\^(60t)

For the sum of 100 independent variables: M_S(t) = (ν / (ν - t))\^100

We minimize the right-hand side with respect to t \< ν.

#### METHOD 3: Central Limit Theorem

**For our problem:**

P(S \> 60) ≥ 0.95

According to the Central Limit Theorem, the sum (or average) of a large number of independent random variables is approximately normally distributed:

S \~ N(μ_S, σ²_S)

where:

-   μ_S = 100 / (N·ν₁)

-   σ_S = 10 / (N·ν₁)

We need to find N such that P(S ≥ 60) ≥ 0.95.

P(S ≥ 60) = P((S - μ_S)/σ_S ≥ (60 - μ_S)/σ_S) ≥ 0.95

=\> P(Z ≥ (60 - μ_S)/σ_S) = 1 - P(Z \< (60 - μ_S)/σ_S)) = 1 - Φ((60 - μ_S)/σ_S) ≥ 0.95\
1 - Φ((60 - μ_S)/σ_S) ≥ 0.95\
Φ((60 - μ_S)/σ_S) ≤ 0.05

Substituting μ_S and σ_S:

(60 - 100/(N·ν₁)) / (10/(N·ν₁)) ≤ -1.645\
(60·N·ν₁ - 100) / 10 ≤ -1.645\
60·N·ν₁ - 100 ≤ -16.45\
N ≤ (100 - 16.45) / (60·ν₁)

N ≤ 83.55 / (60·25) ≤ 0.0555

```{r}
N_markov <- 100 / (3 * v1)
cat("N_max (Markov) =", floor(N_markov), "\n")


chernoff_bound <- function(N, v1, target = 60) {
  v <- N * v1

  t_opt <- v - 100/target  
  if (t_opt >= v || t_opt <= 0) return(Inf)
  
  bound <- ((v/(v - t_opt))^100) * exp(-t_opt * target)
  return(bound)
}

N_test <- 0.1:1
chernoff_values <- sapply(N_test, function(N) chernoff_bound(N, v1))
N_chernoff <- max(N_test[chernoff_values <= 0.05])

cat("N_max (Chernoff) =", N_chernoff, "\n")


N_clt <- (100 - 16.45) / (60 * v1)

cat("N_max (CLT) =", floor(N_clt), "\n")

cat("Method comparison\n")
cat("Markov:   N ≤", N_markov, "\n")
cat("Chernoff: N ≤", N_chernoff, "\n")
cat("CLT:      N ≤", N_clt, "\n")
```

-   simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;

```{r}
N_predicted <- N_clt 
N <-1
v1_c <- v1
v <- N * v1_c

cat(paste("Simulation for N = 1", "with rate v =", round(v, 7), "clicks/с.\n"))

x_realization <- rexp(100, rate = v)
S_realization <- sum(x_realization)

cat(paste("One realization for S = X_1 + ... + X_100:", round(S_realization, 2), "с.\n"))
cat(head(x_realization))
```

-   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;

```{r}
n_clicks <- 100
K <- 1000

s_sample <- colSums(matrix(rexp(n_clicks * K, rate = v), nrow = n_clicks, ncol = K))

cat(head(s_sample))
```

-   estimate the probability that the location is identified as safe and compare to the desired level $0.95$

```{r}
safe_count <- sum(s_sample > 60)

empirical_prob <- safe_count / K 

cat(paste("Number of safe cases (S > 60s):", safe_count, "out of", K, "\n"))
cat(paste("Empirical probability of safety P(S > 60):", empirical_prob, "\n"))
cat(paste("Desired probability level:", 0.95, "\n"))
```

**Conclusion for task 3**

**For part 1:** As n increases (5→10→50), the distribution of means increasingly approaches normal (max_diff decreases). **For part 2:** CLT provides the most practical result for determining a safe number of samples.

So, in general We modeled a Geiger counter with the exponential distribution. We showed that when you average the time between clicks, the average becomes normally distributed if you use a large enough sample size (like 50). This worked well. And also we tried to find the maximum number of radioactive samples that could be stored safely.

------------------------------------------------------------------------

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its moments: expectation and variance.**

    1.  Suppose we have a random variable $X$. Explain why $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

    2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

    ```{r}
    id <- 15
    mu <- id
    sigma <- sqrt(2*id+7)
    N <- 100

    x_sample <- rnorm(N, mean = mu, sd = sigma)

    y_sample <- 1 / x_sample


    mean_X <- mean(x_sample)
    one_over_mean_X <- 1/mean_X
    mean_Y <- mean(y_sample)

    cat("Comparison:\n")
    cat("1/X̄ =", one_over_mean_X, "\n")
    cat("Ȳ =", mean_Y, "\n")
    cat("Difference:", abs(one_over_mean_X - mean_Y))
    ```

    The output clearly shows that the two calculated values are different.

    -   `1/X̄` (1/mean(X)) ≠ `Ȳ` (mean(Y) = mean(1/X))

    This confirms that E[1/X] ≠ 1/E[X].

    3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

    ```{r}
    lambda <- 2
    N <- 1000

    X <- rexp(N, rate = lambda)
    Y <- rexp(N, rate = lambda)
    Z <- log(X) + 5

    par(mfrow = c(1, 2))


    plot(X, Y, 
         main = "Scatterplot: X vs Y (independent)",
         xlab = "X", ylab = "Y", col = "purple")

    qqplot(X, Y, 
         main = "Q-Q Plot: X vs Y (independent)")
    abline(0, 1, col = "pink", lwd = 2)


    plot(X, Z, 
         main = "Scatterplot: X vs Z (not independent)",
         xlab = "X", ylab = "Z = log(X)+5", col = "purple")

    qqplot(X, Z, 
         main = "Q-Q Plot: X vs Z (not indepedent)")
    abline(0, 1, col = "pink", lwd = 2)

    ```

    **Analysis for X and Y:**

    -   **QQ-plot:** The points lie close to the line y=x. → This means X and Y have the **same** distribution (both are Exp(2)).

    -   **Scatterplot:** The points are scattered **chaotically**, without a pattern. → This means X and Y are **independent**.

    **ANALYSIS for X and Z:**

    -   **QQ-plot:** The points do**n\`t** lie on the line y=x. → This means X and Z have **different** distributions .

    -   **Scatterplot:** There is a clear **logarithmic** relationship. → Z=log(X)+5 . → This means X and Z are **dependent**.

    ------------------------------------------------------------------------

2.  You toss a fair coin three times and a random variable $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost £$1$. They will receive £$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

            **So, because of:**

        -   Fixed number of trials: n = 3

        -   Two possible outcomes: Heads or Tails

        -   Constant probability of success: p = 0.5

        -   The trials are independent

        =\> **X \~ Binomial(n=3, p=0.5)**

    2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    n <- 3
    p <- 0.5
    N <- 100

    theoretical_E_X <- n * p
    theoretical_Var_X <- n * p * (1 - p)

    X <- rbinom(N, size = n, prob = p)

    sample_mean_X <- mean(X)
    sample_var_X <- var(X)
    # using formula
    x_bar <- mean(X)
    sample_var_X_method2 <- sum((X - x_bar)^2) / (N - 1)

    cat("Theoretical E[X]:", theoretical_E_X, "\n")
    cat("Sample mean:", sample_mean_X, "\n")
    cat("Theoretical Var(X):", theoretical_Var_X, "\n")
    cat("Sample variance:", sample_var_X, "\n")
    cat("Sample variance using formula:", sample_var_X_method2)
    ```

    The empirical values are close to the theoretical ones. As N increases, they would converge even closer (because of LLN).

    3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    Y <- 0.5 * X - 1

    theoretical_E_Y <- 0.5 * theoretical_E_X - 1
    theoretical_Var_Y <- (0.5^2) * theoretical_Var_X

    sample_mean_Y <- mean(Y)
    sample_var_Y <- var(Y)
    # using formula
    y_bar <- mean(Y)
    sample_var_Y_method2 <- sum((Y - y_bar)^2) / (N - 1)

    cat("Theoretical E[Y]:", theoretical_E_Y, "\n")
    cat("Sample mean:", sample_mean_Y, "\n")
    cat("Theoretical Var(Y):", theoretical_Var_Y, "\n")
    cat("Sample variance:", sample_var_Y, "\n")
    cat("Sample variance using formula", sample_var_Y_method2)
    ```

    The empirical values are close to the theoretical ones. As N increases, they would converge even closer (because of LLN).

    **Conclusion on the game**: Since $E[Y] = -0.25$ , it means that, on average, your friend will **lose £0.25** per game. (as Y = revenue - costs =\> profit = Y = 0.5·X - 1) This is an unprofitable and unfair game for him.

------------------------------------------------------------------------

### **General Summary and Conclusions**

In this lab assignment, we applied several fundamental concepts of probability theory and statistics — including the Law of Large Numbers, the Central Limit Theorem, and different probabilistic bounds — to analyze both discrete and continuous random processes.

In **Task 1**, we simulated the $[7,4]$ Hamming code to study its reliability in noisy transmission. The results confirmed that the estimated probability of correct decoding was almost identical to the theoretical one, and that the confidence interval became narrow for large $N$. The histogram of decoding errors matched expectations for a code that corrects single-bit errors.

In **Task 2**, we modeled radioactive decay using the Poisson distribution. The distribution of sample means became close to the normal one as $n$ increased, confirming the Central Limit Theorem. We also compared the Markov, Chernoff, and CLT bounds for safety conditions, and verified through simulation that Chernoff and CLT provided realistic estimates while Markov was overly conservative.

In **Task 3**, we analyzed the Geiger counter process with exponentially distributed inter-click times. Simulations again demonstrated the CLT in practice — the sample-mean distribution converged to normality as $n$ grew. Theoretical bounds using Markov and CLT were calculated to determine the maximum safe number of radioactive samples. The results showed that even one highly active sample could exceed safety limits, illustrating the strictness of the safety condition.

Finally, in **Task 4**, we explored the relationship between random variables, their independence, and transformations. We showed empirically that $\mathbb{E}(1/X) \neq 1/\mathbb{E}(X)$, demonstrated the difference between independent and dependent random variables using scatterplots and Q–Q plots, and analyzed a coin-toss game as a binomial model. The game’s expected payoff was negative, confirming it is statistically unfair.
